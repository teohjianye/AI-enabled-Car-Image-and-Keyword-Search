{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"keywordsearch.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"pkcGPYo6tuZ3"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from nltk import sent_tokenize\n","import torch\n","from sentence_transformers import SentenceTransformer, util\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zdKGfpzeD0-o"},"source":["#Similarity search for car make model\n","\n","The current problem of the existing SgCarMart search engine is the less accurate search results from users’ inputs that might affect customer experiences. For example, when a user types ‘cc’ in the search box, the returned result is usually Honda Accord at the top, instead of Volkswagen cc.\n","\n","Especially while you type the model, mostly getting the results is not accurate.\n","\n","I hope AI will help us the problem"]},{"cell_type":"markdown","metadata":{"id":"Khcj2PkBEMgM"},"source":["mount the drive"]},{"cell_type":"code","metadata":{"id":"Q5RP2K0Tj3dP"},"source":["df=pd.read_csv('/content/gdrive/My Drive/ISY5002/Project/Image_Search/data/car_make model_2.csv')\n","df['CarModel']= df['CarModel'].apply(lambda x: x.lower())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fep6Cl1aiEns"},"source":["corpus = df['CarModel'].astype(str).values.tolist()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_lu7kBGNFgpd"},"source":["#put the search terms, and corpus\n","base_document = corpus\n","query = 'cc'\n","#query = 'bmw'\n","\n","#threhold\n","highest_score = 1\n","lowest_score = 0.55\n","\n","#top result\n","top_k = 5\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gUcjJUS0vJmK"},"source":["import operator\n","from itertools import islice\n","\n","\n","def get_best_similar(best_suit):\n","  sorted_d = dict( sorted(best_suit.items(), key=operator.itemgetter(1),reverse=True))\n","  return (dict(islice(sorted_d.items(), top_k)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ya85uPsFFM_x"},"source":["BERT\n","\n","We came out with solution to solve this issue is using BERT pre-train model.\n","\n","What is BERT?\n","\n","**BERT** (introduced in [this paper](https://arxiv.org/abs/1810.04805)) stands for Bidirectional Encoder Representations from Transformers. It is a trained Transformer Encoder stack. BERT is a model that broke several records for how well models can handle language-based tasks after the release paper. It is saving the time, energy, knowledge, and resources that would have gone to training a language-processing model from scratch. Its the good point.\n","\n","The paper presents two model sizes for BERT:\n","BERT BASE (L=12, H=768, A=12, Total Parameters=110M) and BERT LARGE (L=24, H=1024, A=16, Total Parameters=340M).In this project, i will use bert-base. \n","\n","\n","more clear explaination in [it](https://jalammar.github.io/illustrated-bert/)"]},{"cell_type":"code","metadata":{"id":"05kIwEfhj7UP"},"source":["def cosine_similarities_test(query_embedding, corpus_embeddings, top_k=5):\n","\tcos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\n","\ttop_results = torch.topk(cos_scores, k=top_k)\n","\tfor score, idx in zip(top_results[0], top_results[1]):\n","\t\tprint(corpus[idx], \"(Score: {:.4f})\".format(score))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aKtvr6W4j_zl"},"source":["def get_bert_similarity(query, base_document):\n","\t# This will download and load the pretrained model offered by UKPLab.\n","\tmodel = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","\trecommendations=[]\n","\n","\t# Although it is not explicitly stated in the official document of sentence transformer, the original BERT is meant for a shorter sentence. We will feed the model by sentences instead of the whole documents.\n","\tbase_embeddings_sentences = model.encode(base_document)\n","\tfor document in [query]:\n","\t\tembeddings_sentences = model.encode(document)\n","\t\tcos_scores = util.pytorch_cos_sim(embeddings_sentences, base_embeddings_sentences)[0]\n","\t\ttop_results = torch.topk(cos_scores, k=top_k)\n","\t\tprint(\"\\n\\n======================\\n\\n\")\n","\t\tprint(\"Query:\", document)\n","\t\tprint(\"\\nTop 5 most similar sentences in corpus:\")\n","\t\tfor score, idx in zip(top_results[0], top_results[1]):\n","\t\t\tif score > 0.5:\n","\t\t\t\trecommendations.append(corpus[idx])\n","\n","\t\tprint(corpus[idx], \"(Score: {:.4f})\".format(score))\n","\n","\n","\n","\treturn recommendations\n"],"execution_count":null,"outputs":[]}]}